{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4152151",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "- **Total Parameters**: 976,219 (~976K)\n",
    "- **Model Size**: 3.72 MB (FP32)\n",
    "- **Complexity**: Medium (efficient for deployment)\n",
    "- **Tasks**: Multi-task learning (Classification + Regression)\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Classification Accuracy | 53.79% (baseline) |\n",
    "| MAE (Regression) | 0.014468 |\n",
    "| RMSE (Regression) | 0.020668 |\n",
    "\n",
    "### Generalization Test Results\n",
    "\n",
    "| Subset | Period | Accuracy | MAE | Notes |\n",
    "|--------|--------|----------|-----|-------|\n",
    "| Subset 1 | 2018-03-01 to 2020-10-29 | 51.66% | 0.0187 | Much earlier period |\n",
    "| Subset 4 | 2018-11-28 to 2021-07-28 | 53.23% | 0.0207 | Earlier period |\n",
    "| Subset 9 | 2020-03-04 to 2022-11-02 | 54.13% | 0.0175 | Overlaps training |\n",
    "| Subset 10 | 2020-06-03 to 2023-02-03 | 52.42% | 0.0154 | Similar timeframe |\n",
    "| **Subset 12** | **2020-12-02 to 2023-08-02** | **53.79%** | **0.0145** | **Training baseline** |\n",
    "| Subset 13 | 2021-03-05 to 2023-11-03 | 55.78% | 0.0137 | **Best - extends beyond** |\n",
    "\n",
    "**Conclusion**: Model shows **good generalization** with only 2.29% avg accuracy deviation across time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b73974",
   "metadata": {},
   "source": [
    "## Architecture Components\n",
    "\n",
    "### 1. Input Processing (152,384 params)\n",
    "\n",
    "```\n",
    "start_emb_l: Low-frequency embedding (363 → 128)\n",
    "start_emb_h: High-frequency embedding (363 → 128)\n",
    "te_emb: Time encoding embedding (55 → 128)\n",
    "```\n",
    "\n",
    "**Purpose**: Transform raw 360 Alpha factors into embedding space, separately for low and high frequencies.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Wavelet Transform Module\n",
    "\n",
    "- **Type**: DWT1DForward (Discrete Wavelet Transform)\n",
    "- **Wavelet**: sym2 (Symlet with 2 vanishing moments)\n",
    "- **Level**: 1 decomposition\n",
    "\n",
    "**Function**: Decomposes time series into:\n",
    "- **Low-frequency (XL)**: Captures trends and smooth patterns\n",
    "- **High-frequency (XH)**: Captures noise and rapid fluctuations\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Dual-Frequency Encoder (660K params)\n",
    "\n",
    "**2 Transformer Layers × 330K params each**\n",
    "\n",
    "Each layer contains:\n",
    "\n",
    "#### a) TCN - Temporal Convolutional Network (32,896 params)\n",
    "- Captures local temporal patterns\n",
    "- Uses dilated convolutions for larger receptive field\n",
    "\n",
    "#### b) Temporal Attention (66,048 params)\n",
    "- Query, Key, Value, Output projections\n",
    "- Feedforward network\n",
    "- Captures time dependencies across the 20-day window\n",
    "\n",
    "#### c) Spatial Attention - Low Frequency (66,305 params)\n",
    "- Models correlations between stocks\n",
    "- Attention over 255 stocks\n",
    "- Focuses on trend relationships\n",
    "\n",
    "#### d) Spatial Attention - High Frequency (66,305 params)\n",
    "- Models high-frequency co-movements\n",
    "- Captures rapid market reactions\n",
    "- Separate from low-freq to avoid noise mixing\n",
    "\n",
    "**Key Design**: Separate processing paths for low and high frequencies prevents noise contamination of trend signals.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Adaptive Fusion Layer (115,088 params)\n",
    "\n",
    "```\n",
    "Cross-attention mechanism:\n",
    "- Query from low-freq\n",
    "- Keys/Values from both low and high freq\n",
    "- Learns optimal weighting automatically\n",
    "```\n",
    "\n",
    "**Purpose**: Intelligently combines low and high frequency representations based on learned importance.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Graph Integration\n",
    "\n",
    "- **Graph Embeddings**: 128-dimensional vectors from Struc2vec\n",
    "- **Graph Structure**: 32,385 edges between 255 stocks\n",
    "- **Method**: Correlation-based adjacency matrix → Struc2vec → GAT\n",
    "\n",
    "**Purpose**: Captures structural similarities between stocks beyond simple correlations.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multi-Task Heads (33,411 params)\n",
    "\n",
    "#### Classification Head (16,770 params)\n",
    "- Input: 128-dim fused features\n",
    "- Output: 2 classes (up/down)\n",
    "- Loss: Cross-entropy\n",
    "\n",
    "#### Regression Head (16,641 params)\n",
    "- Input: 128-dim fused features\n",
    "- Output: 1 value (continuous return)\n",
    "- Loss: MSE\n",
    "\n",
    "**Shared Representations**: Both tasks benefit from the same learned features, improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097c1c5",
   "metadata": {},
   "source": [
    "## Data Flow Diagram\n",
    "\n",
    "```\n",
    "Input: OHLCV Price-Volume Factors\n",
    "         (360 factors × 20 days × 255 stocks)\n",
    "                    |\n",
    "                    v\n",
    "         Wavelet Transform (DWT)\n",
    "                    |\n",
    "         +----------+----------+\n",
    "         |                     |\n",
    "    Low-Freq (XL)         High-Freq (XH)\n",
    "    (trends)              (noise/rapid)\n",
    "         |                     |\n",
    "         v                     v\n",
    "  Embedding (363→128)   Embedding (363→128)\n",
    "         |                     |\n",
    "         v                     v\n",
    "  ┌──────────────────┐  ┌──────────────────┐\n",
    "  │ Layer 1:         │  │ Layer 1:         │\n",
    "  │ - TCN            │  │ - TCN            │\n",
    "  │ - Temporal Attn  │  │ - Temporal Attn  │\n",
    "  │ - Spatial Attn   │  │ - Spatial Attn   │\n",
    "  └──────────────────┘  └──────────────────┘\n",
    "         |                     |\n",
    "         v                     v\n",
    "  ┌──────────────────┐  ┌──────────────────┐\n",
    "  │ Layer 2:         │  │ Layer 2:         │\n",
    "  │ - TCN            │  │ - TCN            │\n",
    "  │ - Temporal Attn  │  │ - Temporal Attn  │\n",
    "  │ - Spatial Attn   │  │ - Spatial Attn   │\n",
    "  └──────────────────┘  └──────────────────┘\n",
    "         |                     |\n",
    "         +----------+----------+\n",
    "                    |\n",
    "                    v\n",
    "          Adaptive Fusion\n",
    "        (Cross-attention)\n",
    "                    |\n",
    "                    v\n",
    "          Fused Features (128-dim)\n",
    "         + Graph Embeddings (128-dim)\n",
    "                    |\n",
    "         +----------+----------+\n",
    "         |                     |\n",
    "         v                     v\n",
    "  Classification         Regression\n",
    "  (2 classes)            (1 value)\n",
    "  up/down                return %\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24781a24",
   "metadata": {},
   "source": [
    "## Parameter Distribution\n",
    "\n",
    "| Component | Parameters | Percentage |\n",
    "|-----------|------------|------------|\n",
    "| Features Module | 942,808 | 96.58% |\n",
    "| Classifier Module | 33,411 | 3.42% |\n",
    "\n",
    "### Why 96.58% in Features?\n",
    "- Most computation goes into learning good representations\n",
    "- Dual encoders (2 layers each) = major parameter sink\n",
    "- Multi-head attention mechanisms are parameter-heavy\n",
    "- Once features are learned, prediction heads are lightweight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b674f",
   "metadata": {},
   "source": [
    "## Key Hyperparameters\n",
    "\n",
    "| Parameter | Value | Notes |\n",
    "|-----------|-------|-------|\n",
    "| Input timesteps (T1) | 20 | 20 days of historical data |\n",
    "| Output timesteps (T2) | 2 | Predict next 2 days |\n",
    "| Transformer layers (L) | 2 | Per frequency pathway |\n",
    "| Attention heads (h) | 1 | Single-head attention |\n",
    "| Embedding dim (d) | 128 | Hidden representation size |\n",
    "| Sparsity ratio (s) | 1.0 | 100% of connections kept |\n",
    "| Batch size | 12 | Training batch size |\n",
    "| Learning rate | 0.001 | Adam optimizer |\n",
    "| Train/Val/Test | 75/12.5/12.5% | Data split |\n",
    "\n",
    "**Note**: Sparsity ratio of 1.0 means no sparsity pruning is applied (all spatial attention connections are used)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a634d",
   "metadata": {},
   "source": [
    "## Input/Output Specifications\n",
    "\n",
    "### Input Tensors\n",
    "\n",
    "| Tensor | Shape | Description |\n",
    "|--------|-------|-------------|\n",
    "| XL | [batch, 20, 255, 363] | Low-frequency decomposition |\n",
    "| XH | [batch, 20, 255, 363] | High-frequency decomposition |\n",
    "| TE | [batch, 20, 255, time_dim] | Time encoding (day/week/month) |\n",
    "| XC | [batch, 20, 255] | Trend indicators (binary up/down) |\n",
    "| bonus_X | [batch, 255, 128] | Graph embeddings per sample |\n",
    "| adjgat | [255, 128] | Graph structure (fixed) |\n",
    "\n",
    "### Output Tensors\n",
    "\n",
    "| Tensor | Shape | Description |\n",
    "|--------|-------|-------------|\n",
    "| Classification | [batch, 2, 255, 2] | Logits for up/down per day |\n",
    "| Regression | [batch, 2, 255] | Predicted returns per day |\n",
    "\n",
    "**Note**: In practice, only the last timestep (T2=2) predictions are used for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f41633",
   "metadata": {},
   "source": [
    "## Questions for Deep Dive\n",
    "\n",
    "### Architecture Questions:\n",
    "1. **Why separate low/high frequency encoders?**\n",
    "   - TODO: Investigate if shared encoder performs worse\n",
    "   - TODO: Ablation study on dual vs single encoder\n",
    "\n",
    "2. **Why only 1 attention head?**\n",
    "   - TODO: Test multi-head attention (h=4, h=8)\n",
    "   - TODO: Check if single head is sufficient for stock data\n",
    "\n",
    "3. **Why T2=2 (predict 2 days ahead)?**\n",
    "   - TODO: Experiment with T2=1, T2=5, T2=10\n",
    "   - TODO: Analyze prediction decay over longer horizons\n",
    "\n",
    "4. **Sparsity ratio = 1.0 (no pruning)?**\n",
    "   - TODO: Test s=0.1, 0.3, 0.5 for efficiency gains\n",
    "   - TODO: Measure impact on accuracy vs speed\n",
    "\n",
    "### Data Questions:\n",
    "5. **Why 360 Alpha factors?**\n",
    "   - TODO: Feature importance analysis\n",
    "   - TODO: Can we reduce to top 100-200 factors?\n",
    "\n",
    "6. **Graph embedding effectiveness?**\n",
    "   - TODO: Ablation study without graph embeddings\n",
    "   - TODO: Compare Struc2vec vs other graph methods (Node2vec, GraphSAGE)\n",
    "\n",
    "### Performance Questions:\n",
    "7. **Why does Subset 13 perform best?**\n",
    "   - TODO: Analyze market conditions in 2021-2023\n",
    "   - TODO: Compare volatility patterns across subsets\n",
    "\n",
    "8. **Classification accuracy ~54% vs random 50%?**\n",
    "   - TODO: Is this typical for stock prediction?\n",
    "   - TODO: Compare with baseline models (LSTM, GRU, simple MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8029b6",
   "metadata": {},
   "source": [
    "## Code to Load and Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd526e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inspection\n",
    "import torch\n",
    "import configparser\n",
    "from Stockformermodel.Multitask_Stockformer_models import Stockformer\n",
    "\n",
    "# Configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config/Multitask_Stock_Subset12.conf')\n",
    "\n",
    "# Model parameters\n",
    "infeature = 363\n",
    "outfea_class = 2\n",
    "outfea_regress = 1\n",
    "T1 = 20\n",
    "T2 = 2\n",
    "L = 2\n",
    "h = 1\n",
    "d = 128\n",
    "s = 1.0\n",
    "\n",
    "# Create model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Stockformer(infeature, h*d, outfea_class, outfea_regress, L, h, d, s, T1, T2, device).to(device)\n",
    "\n",
    "# Load weights\n",
    "model_path = 'cpt/STOCK/saved_model_Multitask_2020-12-02_2023-08-02'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c324348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect specific layers\n",
    "print(\"\\nDual Encoder Structure:\")\n",
    "for i, layer in enumerate(model.features.dual_encoder):\n",
    "    print(f\"\\nLayer {i+1}:\")\n",
    "    print(f\"  TCN: {sum(p.numel() for p in layer.tcn.parameters()):,} params\")\n",
    "    print(f\"  Temporal Attention: {sum(p.numel() for p in layer.tatt.parameters()):,} params\")\n",
    "    print(f\"  Spatial Attention (Low): {sum(p.numel() for p in layer.ssal.parameters()):,} params\")\n",
    "    print(f\"  Spatial Attention (High): {sum(p.numel() for p in layer.ssah.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d970857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add visualization code\n",
    "# - Attention weight heatmaps\n",
    "# - Feature importance analysis\n",
    "# - Prediction distribution plots\n",
    "# - Temporal pattern visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae991b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Paper**: Ma, Bohan; Xue, Yushan; Lu, Yuan & Chen, Jing. (2025). \"Stockformer: A price-volume factor stock selection model based on wavelet transform and multi-task self-attention networks\". *Expert Systems with Applications*, 273, 126803.\n",
    "\n",
    "2. **GitHub**: https://github.com/Eric991005/Multitask-Stockformer\n",
    "\n",
    "3. **Qlib Documentation**: https://github.com/microsoft/qlib\n",
    "\n",
    "4. **Struc2vec**: https://github.com/shenweichen/GraphEmbedding\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**:\n",
    "- [ ] Run ablation studies on architecture components\n",
    "- [ ] Compare with baseline models (LSTM, GRU, Transformer-only)\n",
    "- [ ] Analyze feature importance\n",
    "- [ ] Test hyperparameter variations\n",
    "- [ ] Adapt architecture for NIFTY-200 Indian stocks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
