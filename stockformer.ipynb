{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66958909",
   "metadata": {},
   "source": [
    "Yes, that makes perfect sense! Here's a comprehensive markdown summary to paste into your notebook:\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockformer Reproduction Project\n",
    "\n",
    "## Objective\n",
    "Reproduce the Stockformer paper's daily-rebalanced backtesting experiments on Chinese stock data to verify reported metrics (IC, Sharpe, turnover, drawdown). Once validated, adapt the pipeline to NIFTY-200 (Indian market).\n",
    "\n",
    "## Paper & Resources\n",
    "- **Paper**: [Stockformer: A Price-Volume Factor Stock Selection Model](https://arxiv.org/abs/2401.06139)\n",
    "- **Official Repo**: https://github.com/Eric991005/Multitask-Stockformer\n",
    "- **My Repo**: https://github.com/rajnishahuja/stockformer\n",
    "\n",
    "## Key Insights from Planning\n",
    "\n",
    "### The 14 Models\n",
    "The paper uses **14 independent models** (NOT sequential/transfer learning):\n",
    "- Each trained on a rolling 2-year window\n",
    "- Each has separate validation (4 months) and test (4 months) periods\n",
    "- Covers full 6-year dataset (2018-03 to 2024-03)\n",
    "- Ensures walk-forward validation without look-ahead bias\n",
    "- Tests robustness across different market conditions\n",
    "\n",
    "Example splits:\n",
    "- Subset 1: Train 2018-03 to 2020-03 ‚Üí Test 2020-07 to 2020-11\n",
    "- Subset 12: Train 2020-12 to 2022-12 ‚Üí Test 2023-04 to 2023-08\n",
    "- Subset 14: Latest 2-year window ‚Üí Recent test period\n",
    "\n",
    "### Architecture Overview\n",
    "- **Input**: 360 volume-price factors (from Qlib) + OHLCV data\n",
    "- **Core**: Dual-frequency encoder with wavelet decomposition\n",
    "  - Low-frequency path: Temporal + Sparse Spatial Attention\n",
    "  - High-frequency path: TCN + Sparse Spatial Attention\n",
    "  - Adaptive Fusion layer combines both\n",
    "- **Output**: Multi-task (regression for returns + classification for trend)\n",
    "- **Graph**: 128-dim Struc2vec embeddings from stock correlation structure\n",
    "\n",
    "### Training Strategy\n",
    "- **Single dataset**: ~1-2 hours on A100\n",
    "- **All 14 datasets**: ~12-24 hours total\n",
    "- **Approach**: Start with 1 dataset validation, then scale to all 14\n",
    "\n",
    "---\n",
    "\n",
    "## Current Status (Dec 26, 2025)\n",
    "\n",
    "### ‚úÖ Completed\n",
    "1. Downloaded one dataset: `Stock_CN_2018-03-01_2020-10-29.zip`\n",
    "2. Extracted and verified structure:\n",
    "   - 650 trading days\n",
    "   - ~250 Chinese stocks (CSI-300 type universe)\n",
    "   - 360 factor CSVs in Alpha_360 folder\n",
    "   - All required files present: labels, flow.npz, trend_indicator.npz, correlation matrices\n",
    "\n",
    "### üìÅ Data Structure (Verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Setup Tasks (To Do on A100 Machine)\n",
    "\n",
    "### 1. Clone Official Repository\n",
    "```bash\n",
    "cd /root/stockformer\n",
    "git clone https://github.com/Eric991005/Multitask-Stockformer.git\n",
    "cd Multitask-Stockformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44b1ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Setup Python Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e82c6c",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Create venv\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Install PyTorch with CUDA\n",
    "pip install --upgrade pip\n",
    "pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install other dependencies\n",
    "pip install pytorch-wavelets==1.3.0\n",
    "pip install scikit-learn==1.1.2 numpy==1.24.4 scipy==1.9.3\n",
    "pip install matplotlib==3.7.1 tqdm==4.62.3 statsmodels==0.14.0\n",
    "pip install pandas tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1116c3d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. Organize Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b228356",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Create data directory structure\n",
    "mkdir -p data\n",
    "mv ../Stock_CN_2018-03-01_2020-10-29 data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088e123",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. Configure Training\n",
    "- Edit `config/Multitask_Stock.conf`\n",
    "- Update data paths to point to `data/Stock_CN_2018-03-01_2020-10-29/`\n",
    "- Verify train/val/test split ratios\n",
    "- Check hyperparameters match paper Table 2\n",
    "\n",
    "### 5. Verify Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a615a06",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Quick test to ensure data loads without errors\n",
    "python -c \"from lib.Multitask_Stockformer_utils import StockDataset; print('Data loader OK')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09966c34",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Training Workflow\n",
    "\n",
    "### Phase 1: Single Dataset Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fb1b5",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Run training on one dataset\n",
    "python MultiTask_Stockformer_train.py --config config/Multitask_Stock.conf\n",
    "\n",
    "# Monitor with TensorBoard\n",
    "tensorboard --logdir runs/\n",
    "\n",
    "# Expected outputs:\n",
    "# - Model checkpoint: cpt/[model_name].pt\n",
    "# - Predictions: output/classification_*.csv, output/regression_*.csv\n",
    "# - Logs: log/training.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ff158",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Success criteria:**\n",
    "- Training converges (loss decreases)\n",
    "- Validation IC improves over epochs\n",
    "- No data loading errors\n",
    "\n",
    "### Phase 2: Run Backtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8345d",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Open Jupyter for backtesting\n",
    "jupyter notebook Backtest/Backtest.ipynb\n",
    "\n",
    "# The notebook will:\n",
    "# 1. Load predictions from output/\n",
    "# 2. Rank stocks by predicted returns\n",
    "# 3. Select TopK stocks\n",
    "# 4. Simulate daily rebalancing\n",
    "# 5. Compute metrics: IC, Sharpe, turnover, drawdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b211088",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Expected metrics (paper ranges):**\n",
    "- IC: ~0.05-0.08\n",
    "- Sharpe: ~2.0-3.5\n",
    "- Annualized return: 30-50%\n",
    "- Daily turnover: varies by TopK\n",
    "\n",
    "### Phase 3: Scale to All 14 Datasets\n",
    "Once Phase 1+2 validated:\n",
    "1. Download remaining 13 sub-datasets from [Google Drive](https://drive.google.com/drive/folders/1ZJpjHiIIkjfbtPIcAmi2nfLNv6VC5ym_)\n",
    "2. Train 13 more models (sequential or parallel if multi-GPU)\n",
    "3. Aggregate backtest results across all periods\n",
    "4. Compare to paper's reported metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Key Decisions & Rationale\n",
    "\n",
    "### Why start with 1 dataset?\n",
    "- Validates entire pipeline (data ‚Üí train ‚Üí backtest ‚Üí metrics)\n",
    "- Catches configuration issues early\n",
    "- Takes only 1-2 hours vs 12-24 hours for all 14\n",
    "- Allows iteration if problems found\n",
    "\n",
    "### Why not change hyperparameters?\n",
    "- Goal is **reproduction**, not improvement\n",
    "- Must match paper exactly to verify claims\n",
    "- Only after successful reproduction can we adapt to NIFTY-200\n",
    "\n",
    "### Why not start NIFTY-200 work yet?\n",
    "- Need baseline Chinese stock results first\n",
    "- Establishes ground truth for comparison\n",
    "- Identifies what works before adaptation\n",
    "- Avoids mixing reproduction issues with adaptation issues\n",
    "\n",
    "---\n",
    "\n",
    "## Git Workflow\n",
    "\n",
    "### On This Laptop (Setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cfe0e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### On A100 Machine\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
